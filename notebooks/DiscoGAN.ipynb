{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DiscoGAN.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO9gskf3Dti8nQcJOqHju8X"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRXFg7re6jWw",
        "colab_type": "text"
      },
      "source": [
        "# Configs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbiy5oAQ3fAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "import random\n",
        "import math\n",
        "\n",
        "import os\n",
        "\n",
        "import pickle\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7Q4hbhiI2pY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content\n",
        "!mkdir datasets\n",
        "\n",
        "# Code from https://github.com/phillipi/pix2pix/blob/master/datasets/download_dataset.sh\n",
        "!wget -N https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/edges2shoes.tar.gz \n",
        "!tar -zxvf edges2shoes.tar.gz -C ./datasets/\n",
        "!rm edges2shoes.tar.gz\n",
        "\n",
        "!wget -N https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/edges2handbags.tar.gz\n",
        "!tar -zxvf edges2handbags.tar.gz -C ./datasets/\n",
        "!rm edges2handbags.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNXWsJ0n6hko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "A_DATA_PATH = os.path.dirname('/content/datasets/edges2handbags/')\n",
        "B_DATA_PATH = os.path.dirname('/content/datasets/edges2shoes/')\n",
        "\n",
        "modelName = \"DiscoGAN-shoes2bags\"\n",
        "\n",
        "log_PATH = os.path.join(\"/gdrive/My Drive/notebooks\", \"logs\",\"DiscoGAN\")\n",
        "\n",
        "batch_size = 200\n",
        "instance_norm = True if batch_size==1 else False\n",
        "workers = 2\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "gf_dim = 64\n",
        "df_dim = 64\n",
        "\n",
        "lambda_a =10.0\n",
        "lambda_b =10.0\n",
        "in_w = in_h = 64\n",
        "c_dim = 3\n",
        "\n",
        "learning_rate = 2e-4\n",
        "betas = (0.5,0.999)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "plt.rc('font',size =15)\n",
        "\n",
        "manualSeed = 3734\n",
        "print(\"Random Seed: \",manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJVKCnZWOP4G",
        "colab_type": "text"
      },
      "source": [
        "# Data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vikaljFkA9sc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [\n",
        "     transforms.Resize((in_h,in_w*2)),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5,),(0.5,)),\n",
        "     ]\n",
        ")\n",
        "\n",
        "def transform_inverse (y):\n",
        "  t= None\n",
        "  if y.size()[0]==1:\n",
        "    t=torchvision.transforms.Normalize((-1,),(2,))\n",
        "  else :\n",
        "    t=torchvision.transforms.Normalize((-1,-1,-1),(2,2,2))\n",
        "  return t(y)\n",
        "\n",
        "def batch_transform_inverse(y):\n",
        "  x = y.new(*y.size())\n",
        "  if y.size()[1]==1:\n",
        "    x[:, 0, :, :] = y[:, 0, :, :] * 2 - 1\n",
        "  else:\n",
        "    x[:, 0, :, :] = y[:, 0, :, :] * 2 - 1\n",
        "    x[:, 1, :, :] = y[:, 1, :, :] * 2 - 1 \n",
        "    x[:, 2, :, :] = y[:, 2, :, :] * 2 - 1\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8os9l2wAD3xL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class pix2pixDataset(Dataset):\n",
        "  def __init__(self,root,test=False, transform=None):\n",
        "    self.root_dir = root\n",
        "    self.test= test\n",
        "    self.transform=transform\n",
        "    self.image_list = []\n",
        "\n",
        "    if self.test == True:\n",
        "        self.root_dir = os.path.join( self.root_dir, 'val' )\n",
        "    else:\n",
        "        self.root_dir = os.path.join( self.root_dir, 'train' )\n",
        "    file_names = os.listdir(self.root_dir)\n",
        "    for f in file_names:\n",
        "      path = os.path.join(self.root_dir,f)\n",
        "      self.image_list.append(path)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    path = self.image_list[idx]\n",
        "    image=Image.open(path)\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "    return image[:,:,:in_h],image[:,:,in_h:]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.image_list)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP5mlOSCA_Hz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_a_set = pix2pixDataset(root=A_DATA_PATH,transform=transform)\n",
        "train_a_loader = torch.utils.data.DataLoader(train_a_set,batch_size=batch_size,\n",
        "                                          shuffle =True, num_workers=workers)\n",
        "train_b_set =  pix2pixDataset(root=B_DATA_PATH,transform=transform)\n",
        "train_b_loader = torch.utils.data.DataLoader(train_b_set,batch_size=batch_size,\n",
        "                                          shuffle =True, num_workers=workers)\n",
        "\n",
        "test_a_set =  pix2pixDataset(root=A_DATA_PATH,test=True,transform=transform)\n",
        "test_a_loader = torch.utils.data.DataLoader(test_a_set, batch_size=batch_size,shuffle = False, num_workers=workers)\n",
        "test_b_set = pix2pixDataset(root=B_DATA_PATH,test=True,transform=transform)\n",
        "test_b_loader = torch.utils.data.DataLoader(test_b_set, batch_size=batch_size,shuffle = False, num_workers=workers)\n",
        "\n",
        "train_a_iter = iter(train_a_loader)\n",
        "train_b_iter = iter(train_b_loader)\n",
        "test_a_iter = iter(test_a_loader)\n",
        "test_b_iter = iter(test_b_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwDATODaBz2r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compare_imshow(first_batch, second_batch, first_title=\"first_batch\", second_title =\"second_batch\", nrow=10, third_batch =None, third_title=\"third_batch\"):\n",
        "  # Plot the first batch\n",
        "  plt.figure(figsize=(40,70))\n",
        "  plt.subplot(1,3,1)\n",
        "  plt.axis(\"off\")\n",
        "  plt.title(first_title)\n",
        "  plt.imshow(np.transpose(vutils.make_grid(first_batch, nrow=nrow,padding=2, normalize=True).cpu(),(1,2,0)))\n",
        "  # Plot the fake images from the last epoch\n",
        "  plt.subplot(1,3,2)\n",
        "  plt.axis(\"off\")\n",
        "  plt.title(second_title)\n",
        "  plt.imshow(np.transpose(vutils.make_grid(second_batch, nrow=nrow,padding=2, normalize=True).cpu(),(1,2,0)))\n",
        "\n",
        "  if third_batch is not None:\n",
        "    plt.subplot(1,3,3)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(third_title)\n",
        "    plt.imshow(np.transpose(vutils.make_grid(third_batch, nrow=nrow,padding=2, normalize=True).cpu(),(1,2,0)))\n",
        "\n",
        "\n",
        "_,domain_a_batch = next(train_a_iter)\n",
        "_,domain_b_batch= next(train_b_iter)\n",
        "compare_imshow(domain_a_batch,domain_b_batch,\"domain A(bag)\",\"domain B(shoes)\")\n",
        "print(\"domain A Batch size : %s \"%str(domain_a_batch.size()))\n",
        "print(\"domain B Batch size : %s \"%str(domain_b_batch.size()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXF9Z1hjGRNJ",
        "colab_type": "text"
      },
      "source": [
        "#ops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFCe5DBTRBCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "def conv_bn_layer(in_channels,out_channels,kernel_size,stride=1,padding=0):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels,out_channels,kernel_size,stride=stride,padding=padding,bias=False),\n",
        "        nn.BatchNorm2d(out_channels,momentum=0.1,eps=1e-5),\n",
        "    )\n",
        "def tconv_bn_layer(in_channels,out_channels,kernel_size,stride=1,padding=0):\n",
        "  return nn.Sequential(\n",
        "      nn.ConvTranspose2d(in_channels,out_channels,kernel_size,stride=stride,padding=padding,bias=False),\n",
        "      nn.BatchNorm2d(out_channels,momentum=0.1,eps=1e-5),\n",
        "  )\n",
        "def tconv_layer(in_channels,out_channels,kernel_size,stride=1,padding=0):\n",
        "  return nn.ConvTranspose2d(in_channels,out_channels,kernel_size,stride=stride,padding=padding)\n",
        "\n",
        "def conv_layer(in_channels,out_channels,kernel_size,stride=1,padding=0):\n",
        "    return nn.Conv2d(in_channels,out_channels,kernel_size,stride=stride,padding=padding)\n",
        "\n",
        "def fc_layer(in_features,out_features):\n",
        "  return nn.Linear(in_features,out_features)\n",
        "\n",
        "def fc_bn_layer(in_features,out_features):\n",
        "  return nn.Sequential(\n",
        "      nn.Linear(in_features,out_features,bias=False),\n",
        "      nn.BatchNorm1d(out_features)\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ4gXJAHRClv",
        "colab_type": "text"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpRkf2zAZFH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DCGAN based Encoder-Decoder Generator refer to https://github.com/SKTBrain/DiscoGAN/blob/master/discogan/model.py\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, extra_layers = False):\n",
        "    super(Generator, self).__init__()\n",
        "    encoder_list = []\n",
        "    basic_layers = 4\n",
        "    # Encoder \n",
        "    in_dim = c_dim\n",
        "    out_dim = gf_dim\n",
        "    encoder_list += nn.Sequential(\n",
        "        conv_layer(in_dim,out_dim,4,2,1),\n",
        "        nn.LeakyReLU(0.2,inplace=True)\n",
        "        )\n",
        "    for i in range(basic_layers-1):\n",
        "      in_dim = out_dim\n",
        "      out_dim = out_dim*2\n",
        "      encoder_list += nn.Sequential(\n",
        "          conv_bn_layer(in_dim,out_dim,4,2,1),\n",
        "          nn.LeakyReLU(0.2,inplace=True)\n",
        "      )\n",
        "    if extra_layers == True:\n",
        "      in_dim = out_dim\n",
        "      out_dim = 100\n",
        "      encoder_list +=  nn.Sequential(\n",
        "          conv_bn_layer(in_dim, out_dim,4,1,0),\n",
        "          nn.LeakyReLU(0.2, inplace=True),\n",
        "      )\n",
        "    self.encoder = nn.Sequential(*encoder_list)\n",
        "\n",
        "    #Decoder\n",
        "    decoder_list = []\n",
        "    if extra_layers == True:\n",
        "      in_dim = out_dim\n",
        "      out_dim = gf_dim * 8 \n",
        "      decoder_list += nn.Sequential(\n",
        "          tconv_bn_layer(in_dim, out_dim, 4,1,0),\n",
        "          nn.ReLU(True),\n",
        "      )\n",
        "    for i in range(basic_layers-1):\n",
        "      in_dim = out_dim\n",
        "      out_dim = int(out_dim/2)\n",
        "      decoder_list += nn.Sequential(\n",
        "          tconv_bn_layer(in_dim, out_dim, 4,2,1),\n",
        "          nn.ReLU(True),\n",
        "      )\n",
        "    in_dim = out_dim\n",
        "    out_dim = c_dim\n",
        "    decoder_list += nn.Sequential(\n",
        "        tconv_layer(in_dim,out_dim,4,2,1),\n",
        "        nn.Sigmoid()\n",
        "      )\n",
        "    self.decoder = nn.Sequential(*decoder_list)\n",
        "  def forward(self, x):\n",
        "    x= self.encoder(x)\n",
        "    x= self.decoder(x)\n",
        "    return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9zOMjNIeGK5W",
        "colab": {}
      },
      "source": [
        "# DCGAN based Discriminator refer to https://github.com/SKTBrain/DiscoGAN/blob/master/discogan/model.py\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self,):\n",
        "    super(Discriminator,self).__init__()\n",
        "    self.conv1 = conv_layer(c_dim,df_dim,4,2,1)\n",
        "    self.conv2 = conv_bn_layer(df_dim,df_dim*2,4,2,1)\n",
        "    self.conv3 = conv_bn_layer(df_dim*2,df_dim*4,4,2,1)\n",
        "    self.conv4 = conv_bn_layer(df_dim*4,df_dim*8,4,2,1)\n",
        "    self.conv5 = conv_layer(df_dim*8,1,4,1,0)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.leaky_relu(self.conv1(x),0.2,inplace=True)\n",
        "    x = F.leaky_relu(self.conv2(x),0.2,inplace=True)\n",
        "    x = F.leaky_relu(self.conv3(x),0.2,inplace=True)\n",
        "    x = F.leaky_relu(self.conv4(x),0.2,inplace=True)\n",
        "    x = self.conv5(x)\n",
        "\n",
        "    return self.sigmoid(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps3jp7QCbTfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weights_init(m):\n",
        "  classname =m.__class__.__name__\n",
        "  if classname.find('Conv')!=-1 : # for Conv\n",
        "    nn.init.normal_(m.weight.data,0.0,0.02)\n",
        "    # nn.init.constant_(m.bias.data,0)\n",
        "  elif classname.find('BatchNorm')!=-1:# Reference : https://discuss.pytorch.org/t/weight-initialization-for-batchnorm-in-dcgan-tutorial/32351\n",
        "    nn.init.normal_(m.weight.data,1.0,0.02)\n",
        "    # nn.init.constant_(m.bias.data,0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4Kkp2nCOUi1",
        "colab_type": "text"
      },
      "source": [
        "# Train configs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhSKzczUZ5op",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "G_AB = Generator(extra_layers=True).to(device)\n",
        "G_BA = Generator(extra_layers=True).to(device)\n",
        "D_A = Discriminator().to(device)\n",
        "D_B = Discriminator().to(device)\n",
        "\n",
        "d = nn.MSELoss()\n",
        "bce = nn.BCELoss()\n",
        "\n",
        "G_AB_optimizer = optim.Adam(G_AB.parameters(), lr=learning_rate,betas=betas)\n",
        "G_BA_optimizer = optim.Adam(G_BA.parameters(), lr=learning_rate,betas=betas)\n",
        "D_A_optimizer = optim.Adam(D_A.parameters(), lr=learning_rate,betas=betas)\n",
        "D_B_optimizer = optim.Adam(D_B.parameters(), lr=learning_rate,betas=betas)\n",
        "\n",
        "# fixed_a_condition, _ = next(iter(test_a_loader))\n",
        "fixed_a_condition = None\n",
        "fixed_b_condition = None\n",
        "\n",
        "for i, (_, t) in enumerate(test_a_loader):\n",
        "  if i ==0:\n",
        "    fixed_a_condition = t\n",
        "  elif i ==4:\n",
        "    break\n",
        "  else:\n",
        "    fixed_a_condition=torch.cat((fixed_a_condition,t),0)\n",
        "\n",
        "for i, (_, t) in enumerate(test_b_loader):\n",
        "  if i ==0:\n",
        "    fixed_b_condition = t\n",
        "  elif i ==4:\n",
        "    break\n",
        "  else:\n",
        "    fixed_b_condition=torch.cat((fixed_b_condition,t),0)\n",
        "\n",
        "\n",
        "fixed_a_condition = fixed_a_condition.to(device)\n",
        "fixed_b_condition = fixed_b_condition.to(device)\n",
        "\n",
        "print(D_A.apply(weights_init))\n",
        "print(D_B.apply(weights_init))\n",
        "print(G_AB.apply(weights_init))\n",
        "print(G_BA.apply(weights_init))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7weEKIGZp9B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "  fake_batch=G_AB(fixed_a_condition)\n",
        "  fake_recon_batch=G_BA(fake_batch)\n",
        "compare_imshow(fixed_a_condition,fake_batch,first_title=\"bag\",second_title=\"bag->shoes\",third_batch =fake_recon_batch,third_title=\"bag->shoes->bag\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_oe_LPsOYyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "  fake_batch=G_BA(fixed_b_condition)\n",
        "  fake_recon_batch = G_AB(fake_batch)\n",
        "compare_imshow(fixed_b_condition,fake_batch,first_title=\"shoes\",second_title=\"shoes->bag\",third_batch = fake_recon_batch, third_title=\"shoes->bag->shoes\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvmwznyAL7bL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_a_list = []\n",
        "img_b_list = []\n",
        "D_A_GAN_losses = []\n",
        "D_B_GAN_losses = []\n",
        "G_AB_GAN_losses = []\n",
        "G_BA_GAN_losses = []\n",
        "\n",
        "recon_A_losses = []\n",
        "recon_B_losses = []\n",
        "\n",
        "iter_per_plot = 500\n",
        "plot_per_eps=(int(len(train_a_loader)/iter_per_plot))\n",
        "transform_PIL=transforms.ToPILImage()\n",
        "\n",
        "train_data_length = len(train_a_loader) if len(train_a_loader) < len(train_b_loader) else len(train_b_loader)\n",
        "\n",
        "def log_list_save(l,file_name):\n",
        "  with open(os.path.join(log_PATH ,file_name+\".logs\"), \"wb\") as fp:\n",
        "    pickle.dump(l, fp)\n",
        "\n",
        "def log_list_load(file_name):\n",
        "  with open(os.path.join(log_PATH ,file_name+\".logs\"), \"rb\") as fp:\n",
        "    return pickle.load(fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ8K7V4dOlTN",
        "colab_type": "text"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul0yNijKFWff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for ep in range(epochs):\n",
        "  for ((i, (_, a_data)), (_, b_data)) in zip(enumerate(train_a_loader), train_b_loader):\n",
        "    b_size= a_data.shape[0]\n",
        "    x_A=a_data.to(device)\n",
        "    x_B=b_data.to(device)\n",
        "\n",
        "    real_label = torch.ones(b_size).to(device)\n",
        "    fake_label = torch.zeros(b_size).to(device)\n",
        "\n",
        "    break\n",
        "\n",
        "    #Train D\n",
        "    ## D_A\n",
        "    D_A.zero_grad()\n",
        "\n",
        "    x_BA = G_BA(x_B)\n",
        "    d_a_fake=D_A(x_BA.detach())\n",
        "    d_a_real=D_A(x_A)\n",
        "    D_A_GAN_loss = bce(d_a_fake,fake_label) + bce(d_a_real,real_label)\n",
        "    D_A_GAN_loss.backward()\n",
        "    D_A_optimizer.step()\n",
        "\n",
        "    ## D_B\n",
        "    D_B.zero_grad()\n",
        "\n",
        "    x_AB = G_AB(x_B)\n",
        "    d_b_fake=D_A(x_BA.detach())\n",
        "    d_b_real=D_A(x_B)\n",
        "    D_B_GAN_loss = bce(d_b_fake,fake_label) + bce(d_b_real,real_label)\n",
        "    D_B_GAN_loss.backward()\n",
        "    D_B_optimizer.step()\n",
        "\n",
        "\n",
        "    ## G with GAN loss\n",
        "    G_AB.zero_grad()\n",
        "    d_b_fake = D_B(fake_A)\n",
        "    G_AB_GAN_loss=bce(d_b_fake,real_label)\n",
        "\n",
        "    G_BA.zero_grad()\n",
        "    d_a_fake = D_A(fake_A)\n",
        "    G_BA_GAN_loss=bce(d_a_fake,real_label)\n",
        "    \n",
        "    # G with Reconstruction loss\n",
        "    ## A->B->A\n",
        "    x_ABA = G_BA(x_AB)\n",
        "    A_recon =d(x_ABA,x_A)\n",
        "    \n",
        "    ## B->A->B\n",
        "    x_BAB= G_AB(x_BA)\n",
        "    B_recon = d(x_BAB,x_B)\n",
        "\n",
        "    G_loss= G_AB_GAN_loss +  G_BA_GAN_loss + A_recon + B_recon\n",
        "    G_loss.backward()\n",
        "    G_AB_optimizer.step()\n",
        "    G_BA_optimizer.step()\n",
        "\n",
        "    if (i+1)%iter_per_plot == 0 :\n",
        "      print('Epoch [{}/{}], Step [{}/{}], D_A_loss: {:.4f}, D_B_loss: {:.4f},G_AB_loss: {:.4f}, G_BA_loss:{:.4f},A_recon_loss:{:.4f},B_recon_loss:{:.4f}' \n",
        "            .format(ep, epochs, i+1, len(train_a_loader), D_A_GAN_loss.item(), D_B_GAN_loss.item(),G_AB_GAN_loss.item(), G_BA_GAN_loss.item(),A_recon.item(),B_recon.item()))\n",
        "      D_A_GAN_losses.append(D_A_GAN_loss.item())\n",
        "      D_B_GAN_losses.append(D_B_GAN_loss.item())\n",
        "      G_AB_GAN_losses.append(G_AB_GAN_loss.item())\n",
        "      G_BA_GAN_losses.append(G_BA_GAN_loss.item())\n",
        "\n",
        "      recon_A_losses.append(A_recon.item())\n",
        "      recon_B_losses.append(B_recon.item())\n",
        "\n",
        "      with torch.no_grad():\n",
        "        G_A.eval()\n",
        "        G_B.eval()\n",
        "        fake_B = G_A(fixed_a_condition).detach()\n",
        "        fake_B_A = G_B(fake_B).detach()\n",
        "        fake_A = G_B(fixed_b_condition).detach()\n",
        "        fake_A_B = G_A(fake_A).detach()\n",
        "        G_A.train()\n",
        "        G_B.train()\n",
        "      figs=plt.figure(figsize=(40,70))\n",
        "      plt.subplot(1,3,1)\n",
        "      plt.axis(\"off\")\n",
        "      plt.title(\"bag\")\n",
        "      plt.imshow(np.transpose(vutils.make_grid(fixed_a_condition, nrow=1,padding=5, normalize=True).cpu(),(1,2,0)))\n",
        "      plt.subplot(1,3,2)\n",
        "      plt.axis(\"off\")\n",
        "      plt.title(\"bag -> shoes\")\n",
        "      plt.imshow(np.transpose(vutils.make_grid(fake_B, nrow=1,padding=5, normalize=True).cpu(),(1,2,0)))\n",
        "      plt.subplot(1,3,3)\n",
        "      plt.axis(\"off\")\n",
        "      plt.title(\"bag -> shoes -> bag\")\n",
        "      plt.imshow(np.transpose(vutils.make_grid(fake_B_A, nrow=1,padding=5, normalize=True).cpu(),(1,2,0)))\n",
        "      plt.savefig(os.path.join(log_PATH,modelName+\"A-\"+str(ep) +\".png\"))\n",
        "      plt.close()\n",
        "      img_a_list.append(figs)\n",
        "\n",
        "      figs=plt.figure(figsize=(40,70))\n",
        "      plt.subplot(1,3,1)\n",
        "      plt.axis(\"off\")\n",
        "      plt.title(\"shoes\")\n",
        "      plt.imshow(np.transpose(vutils.make_grid(fixed_b_condition, nrow=1,padding=5, normalize=True).cpu(),(1,2,0)))\n",
        "      plt.subplot(1,3,2)\n",
        "      plt.axis(\"off\")\n",
        "      plt.title(\"shoes -> bag\")\n",
        "      plt.imshow(np.transpose(vutils.make_grid(fake_A, nrow=1,padding=5, normalize=True).cpu(),(1,2,0)))\n",
        "      plt.subplot(1,3,3)\n",
        "      plt.axis(\"off\")\n",
        "      plt.title(\"shoes -> bag -> shoes\")\n",
        "      plt.imshow(np.transpose(vutils.make_grid(fake_A_B, nrow=1,padding=5, normalize=True).cpu(),(1,2,0)))\n",
        "      plt.savefig(os.path.join(log_PATH,modelName+\"B-\"+str(ep) +\".png\"))\n",
        "      plt.close()\n",
        "      img_b_list.append(figs)\n",
        "\n",
        "      log_list_save(D_A_GAN_losses,\"D_A_GAN_losses\")\n",
        "      log_list_save(D_B_GAN_losses,\"D_B_GAN_losses\")\n",
        "      log_list_save(G_AB_GAN_losses,\"G_AB_GAN_losses\")\n",
        "      log_list_save(G_BA_GAN_losses,\"G_BA_GAN_losses\")\n",
        "      log_list_save(recon_A_losses,\"recon_A_losses\")\n",
        "      log_list_save(recon_B_losses,\"recon_B_losses\") \n",
        "\n",
        "      torch.save(D_A.state_dict(),os.path.join(log_PATH,\"D_A_\"+modelName+\".pth\"))\n",
        "      torch.save(D_B.state_dict(),os.path.join(log_PATH,\"D_B_\"+modelName+\".pth\"))\n",
        "      torch.save(G_A.state_dict(),os.path.join(log_PATH,\"G_AB_\"+modelName+\".pth\"))\n",
        "      torch.save(G_B.state_dict(),os.path.join(log_PATH,\"G_BA_\"+modelName+\".pth\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cJdfVfrY-KW",
        "colab_type": "text"
      },
      "source": [
        "## Test and plot logs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpZQZBr0OC6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "  fake_batch=G_AB(fixed_a_condition)\n",
        "  fake_recon_batch=G_BA(fake_batch)\n",
        "compare_imshow(fixed_a_condition,fake_batch,first_title=\"bag\",second_title=\"bag->shoes\",third_batch =fake_recon_batch,third_title=\"bag->shoes->bag\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6OE6BrQPEJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "  fake_batch=G_BA(fixed_b_condition)\n",
        "  fake_recon_batch = G_AB(fake_batch)\n",
        "compare_imshow(fixed_b_condition,fake_batch,first_title=\"shoes\",second_title=\"shoes->bag\",third_batch = fake_recon_batch, third_title=\"shoes->bag->shoes\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Kyo23NmPHLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.title(\"bag -> shoes Gan loss\")\n",
        "\n",
        "epsilon = 1/plot_per_eps\n",
        "X = np.array(range(plot_per_eps*epochs))/plot_per_eps\n",
        "plt.rc('font',size =9)\n",
        "plt.plot(X,D_A_GAN_losses,label=\"G loss\")\n",
        "plt.plot(X,G_A_GAN_losses,label=\"D loss\")\n",
        "plt.legend(loc=2)\n",
        "plt.xticks(np.arange(0,epochs+1,20)) \n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.savefig(os.path.join(log_PATH,modelName+\"s2b_loss_figure.png\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZThZdhaPXcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.title(\"shoes -> bag Gan loss\")\n",
        "\n",
        "epsilon = 1/plot_per_eps\n",
        "X = np.array(range(plot_per_eps*epochs))/plot_per_eps\n",
        "plt.rc('font',size =9)\n",
        "plt.plot(X,D_B_GAN_losses,label=\"G loss\")\n",
        "plt.plot(X,G_BA_GAN_losses,label=\"D loss\")\n",
        "plt.legend(loc=2)\n",
        "plt.xticks(np.arange(0,epochs+1,20)) \n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.savefig(os.path.join(log_PATH,modelName+\"s2b_loss_figure.png\"))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArsJgQ1pPjyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.title(\"cycle consistency loss\")\n",
        "\n",
        "epsilon = 1/plot_per_eps\n",
        "X = np.array(range(plot_per_eps*epochs))/plot_per_eps\n",
        "plt.rc('font',size =9)\n",
        "plt.plot(X,recon_A_losses,label=\"h->z->h cycle loss\")\n",
        "plt.plot(X,recon_B_losses,label=\"z->h->z cycle loss\")\n",
        "plt.legend(loc=2)\n",
        "plt.xticks(np.arange(0,epochs+1,20)) \n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.savefig(os.path.join(log_PATH,modelName+\"cycle_loss_figure.png\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxiX0THwQA4b",
        "colab_type": "text"
      },
      "source": [
        "# Model save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J2EwzBf3xgg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(D_A.state_dict(),os.path.join(log_PATH,\"D_A_\"+modelName+\".pth\"))\n",
        "torch.save(D_B.state_dict(),os.path.join(log_PATH,\"D_B_\"+modelName+\".pth\"))\n",
        "torch.save(G_A.state_dict(),os.path.join(log_PATH,\"G_AB_\"+modelName+\".pth\"))\n",
        "torch.save(G_B.state_dict(),os.path.join(log_PATH,\"G_BA_\"+modelName+\".pth\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43p3D5qGA0Mp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "D_A.load_state_dict(torch.load(os.path.join(log_PATH,\"D_A_\"+modelName+\".pth\")))\n",
        "D_B.load_state_dict(torch.load(os.path.join(log_PATH,\"D_B_\"+modelName+\".pth\")))\n",
        "G_A.load_state_dict(torch.load(os.path.join(log_PATH,\"G_AB_\"+modelName+\".pth\")))\n",
        "G_B.load_state_dict(torch.load(os.path.join(log_PATH,\"G_BA_\"+modelName+\".pth\")))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}